<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>groupId</groupId>
    <artifactId>SparkKafkaTest</artifactId>
    <version>3.1.2</version>
    <packaging>jar</packaging>
    <name>Spark Project Examples</name>
    <url>http://spark.apache.org/</url>

    <properties>
        <sbt.project.name>examples</sbt.project.name>
        <build.testJarPhase>none</build.testJarPhase>
        <build.copyDependenciesPhase>package</build.copyDependenciesPhase>
        <hadoop.deps.scope>compile</hadoop.deps.scope>
        <hive.deps.scope>compile</hive.deps.scope>
        <parquet.deps.scope>compile</parquet.deps.scope>
            <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
            <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
            <java.version>1.8</java.version>
            <maven.compiler.source>${java.version}</maven.compiler.source>
            <maven.compiler.target>${java.version}</maven.compiler.target>
            <maven.version>3.6.3</maven.version>
            <exec-maven-plugin.version>1.6.0</exec-maven-plugin.version>
            <sbt.project.name>spark</sbt.project.name>
            <slf4j.version>1.7.30</slf4j.version>
            <log4j.version>1.2.17</log4j.version>
            <hadoop.version>3.2.2</hadoop.version>
            <protobuf.version>2.5.0</protobuf.version>
            <yarn.version>${hadoop.version}</yarn.version>
            <zookeeper.version>3.6.2</zookeeper.version>
            <curator.version>2.13.0</curator.version>
            <hive.group>org.apache.hive</hive.group>
            <hive.classifier>core</hive.classifier>
            <!-- Version used in Maven Hive dependency -->
            <hive.version>2.3.8</hive.version>
            <hive23.version>2.3.8</hive23.version>
            <!-- Version used for internal directory structure -->
            <hive.version.short>2.3</hive.version.short>
            <!-- note that this should be compatible with Kafka brokers version 0.10 and up -->
            <kafka.version>2.6.0</kafka.version>
            <!-- After 10.15.1.3, the minimum required version is JDK9 -->
            <derby.version>10.14.2.0</derby.version>
            <parquet.version>1.11.1</parquet.version>
            <orc.version>1.6.7</orc.version>
            <jetty.version>9.4.37.v20210219</jetty.version>
            <jakartaservlet.version>4.0.3</jakartaservlet.version>
            <chill.version>0.9.5</chill.version>
            <ivy.version>2.4.0</ivy.version>
            <oro.version>2.0.8</oro.version>
            <!--
            If you changes codahale.metrics.version, you also need to change
            the link to metrics.dropwizard.io in docs/monitoring.md.
            -->
            <codahale.metrics.version>4.1.1</codahale.metrics.version>
            <avro.version>1.10.2</avro.version>
            <aws.kinesis.client.version>1.14.0</aws.kinesis.client.version>
            <!-- Should be consistent with Kinesis client dependency -->
            <aws.java.sdk.version>1.11.844</aws.java.sdk.version>
            <!-- the producer is used in tests -->
            <aws.kinesis.producer.version>0.12.8</aws.kinesis.producer.version>
            <!--  org.apache.httpcomponents/httpclient-->
            <commons.httpclient.version>4.5.13</commons.httpclient.version>
            <commons.httpcore.version>4.4.12</commons.httpcore.version>
            <!--  commons-httpclient/commons-httpclient-->
            <httpclient.classic.version>3.1</httpclient.classic.version>
            <commons.math3.version>3.4.1</commons.math3.version>
            <!-- managed up from 3.2.1 for SPARK-11652 -->
            <commons.collections.version>3.2.2</commons.collections.version>
            <scala.version>2.12.10</scala.version>
            <scala.binary.version>2.12</scala.binary.version>
            <scalatest-maven-plugin.version>2.0.0</scalatest-maven-plugin.version>
            <scalafmt.parameters>--test</scalafmt.parameters>
            <!-- for now, not running scalafmt as part of default verify pipeline -->
            <scalafmt.skip>true</scalafmt.skip>
            <codehaus.jackson.version>1.9.13</codehaus.jackson.version>
            <fasterxml.jackson.version>2.12.2</fasterxml.jackson.version>
            <snappy.version>1.1.8.2</snappy.version>
            <netlib.java.version>1.1.2</netlib.java.version>
            <commons-codec.version>1.15</commons-codec.version>
            <commons-compress.version>1.20</commons-compress.version>
            <commons-io.version>2.8.0</commons-io.version>
            <!-- org.apache.commons/commons-lang/-->
            <commons-lang2.version>2.6</commons-lang2.version>
            <!-- org.apache.commons/commons-lang3/-->
            <commons-lang3.version>3.11</commons-lang3.version>
            <!-- org.apache.commons/commons-pool2/-->
            <commons-pool2.version>2.6.2</commons-pool2.version>
            <datanucleus-core.version>4.1.17</datanucleus-core.version>
            <guava.version>14.0.1</guava.version>
            <janino.version>3.0.16</janino.version>
            <jersey.version>2.30</jersey.version>
            <joda.version>2.10.5</joda.version>
            <jodd.version>3.5.2</jodd.version>
            <jsr305.version>3.0.0</jsr305.version>
            <libthrift.version>0.12.0</libthrift.version>
            <antlr4.version>4.8-1</antlr4.version>
            <jpam.version>1.1</jpam.version>
            <selenium.version>3.141.59</selenium.version>
            <htmlunit.version>2.40.0</htmlunit.version>
            <maven-antrun.version>1.8</maven-antrun.version>
            <commons-crypto.version>1.1.0</commons-crypto.version>
            <commons-cli.version>1.2</commons-cli.version>
            <!--
            If you are changing Arrow version specification, please check
            ./python/pyspark/sql/pandas/utils.py, and ./python/setup.py too.
            -->
            <arrow.version>2.0.0</arrow.version>
            <!-- org.fusesource.leveldbjni will be used except on arm64 platform. -->
            <leveldbjni.group>org.fusesource.leveldbjni</leveldbjni.group>

            <test.java.home>${java.home}</test.java.home>
            <project.version>3.1.2</project.version>
            <!-- Some UI tests require Chrome and Chrome driver installed so those tests are disabled by default. -->
            <test.default.exclude.tags>org.apache.spark.tags.ChromeUITest</test.default.exclude.tags>
            <test.exclude.tags></test.exclude.tags>
            <test.include.tags></test.include.tags>

            <test.jdwp.address>localhost:0</test.jdwp.address>
            <test.jdwp.suspend>y</test.jdwp.suspend>
            <test.jdwp.server>y</test.jdwp.server>
            <test.debug.suite>false</test.debug.suite>

            <!-- Package to use when relocating shaded classes. -->
            <spark.shade.packageName>org.sparkproject</spark.shade.packageName>

            <!-- Modules that copy jars to the build directory should do so under this location. -->
            <jars.target.dir>${project.build.directory}/scala-${scala.binary.version}/jars</jars.target.dir>

            <!-- Allow modules to enable / disable certain build plugins easily. -->
            <build.testJarPhase>prepare-package</build.testJarPhase>
            <build.copyDependenciesPhase>none</build.copyDependenciesPhase>

            <!--
              Dependency scopes that can be overridden by enabling certain profiles. These profiles are
              declared in the projects that build assemblies.
              For other projects the scope should remain as "compile", otherwise they are not available
              during compilation if the dependency is transitive (e.g. "graphx/" depending on "core/" and
              needing Hadoop classes in the classpath to compile).
            -->
            <hadoop.deps.scope>compile</hadoop.deps.scope>
            <hive.deps.scope>compile</hive.deps.scope>
            <hive.storage.version>2.7.2</hive.storage.version>
            <hive.storage.scope>compile</hive.storage.scope>
            <hive.common.scope>compile</hive.common.scope>
            <hive.llap.scope>compile</hive.llap.scope>
            <hive.serde.scope>compile</hive.serde.scope>
            <hive.shims.scope>compile</hive.shims.scope>
            <orc.deps.scope>compile</orc.deps.scope>
            <parquet.deps.scope>compile</parquet.deps.scope>
            <parquet.test.deps.scope>test</parquet.test.deps.scope>

            <!--
              These default to Hadoop 3.x shaded client/minicluster jars, but are switched to hadoop-client
              when the Hadoop profile is hadoop-2.7, because these are only available in 3.x. Note that,
              as result we have to include the same hadoop-client dependency multiple times in hadoop-2.7.
            -->
            <hadoop-client-api.artifact>hadoop-client-api</hadoop-client-api.artifact>
            <hadoop-client-runtime.artifact>hadoop-client-runtime</hadoop-client-runtime.artifact>
            <hadoop-client-minicluster.artifact>hadoop-client-minicluster</hadoop-client-minicluster.artifact>

            <!--
              Overridable test home. So that you can call individual pom files directly without
              things breaking.
            -->
            <spark.test.webdriver.chrome.driver></spark.test.webdriver.chrome.driver>
            <spark.test.docker.keepContainer>false</spark.test.docker.keepContainer>

            <CodeCacheSize>1g</CodeCacheSize>
            <!-- Needed for consistent times -->
            <maven.build.timestamp.format>yyyy-MM-dd HH:mm:ss z</maven.build.timestamp.format>
    </properties>

    <dependencies>
        <!-- Prevent our dummy JAR from being included in Spark distributions or uploaded to YARN -->
        <dependency>
            <groupId>org.spark-project.spark</groupId>
            <artifactId>unused</artifactId>
            <version>1.0.0</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-graphx_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql-kafka-0-10_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
            <scope>compile</scope>
        </dependency>
        <dependency>
            <groupId>com.github.scopt</groupId>
            <artifactId>scopt_${scala.binary.version}</artifactId>
            <version>3.7.1</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.maven.plugins/maven-jar-plugin -->
        <dependency>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-jar-plugin</artifactId>
            <version>3.2.2</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.maven.plugins/maven-compiler-plugin -->
        <dependency>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>3.10.1</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.maven.plugins/maven-deploy-plugin -->
        <dependency>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-deploy-plugin</artifactId>
            <version>3.0.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.maven.plugins/maven-install-plugin -->
        <dependency>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-install-plugin</artifactId>
            <version>3.0.1</version>
        </dependency>

    </dependencies>

    <build>
        <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>
        <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-deploy-plugin</artifactId>
                <configuration>
                    <skip>true</skip>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-install-plugin</artifactId>
                <configuration>
                    <skip>true</skip>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <configuration>
                    <outputDirectory>${jars.target.dir}</outputDirectory>
                </configuration>
            </plugin>
        </plugins>
    </build>

</project>
